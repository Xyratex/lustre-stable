Overview
--------

This survey script does sequential I/O with varying numbers of threads and
objects (files) by using lctl to drive the echo_client connected
to local or remote obdfilter instances, or remote obdecho instances.

It can be used to characterise the performance of the following lustre
components.

1. The Object Storage Targets.

   Here the script directly exercises one or more instances of obdfilter.
   They may be running on 1 or more nodes, e.g. when they are all attached
   to the same multi-ported disk subsystem.

2. The Network.

   Here the script drives one instance of obdecho server via an instance
   of echo_client running on 1 node.

3. The Stripe F/S over the Network.

   Here the script drives one or more instances of obdfilter via instances
   of echo_client running on a Lustre client node.

Note that the script is _NOT_ scalable to 100s of nodes since it is only
intended to measure individual servers, not the scalability of the system
as a whole.

Running
-------

The script must be customised according to the components under test and
where it should keep its working files. Customization variables are
described clearly at Customization variables Section in the script.
Please see maximum suported value ranges for customization variables
in the script.

In all cases the script will attempt to load the obdecho module where it is
needed.

In "disk" and "netdisk" cases a Lustre configuration should be created using your
normal methods. In case "network" no Lustre configuration is needed.
A mounted Lustre client is only required for case=netdisk.

To profile Object Storage Targets directly
------------------------------------------
Important Note: Remember, write tests are destructive! This test should be run
prior to startup of your actual Lustre filesystem. If that is the case, you
will not need to reformat to restart Lustre - however, if the test is terminated
before completion, you may have to remove objects from the disk.


1. Automated run (local OSTs only):
- Create a Lustre configuraton using your normal methods. No Lustre clients
need be mounted.
Invoke obdfilter-survey from the OSS node whose OST(s) you wish to exercise,
and specify any customization variables at the command line.
e.g. : $ nobjhi=2 thrhi=2 size=1024 case=disk sh obdfilter-survey


2. Manual run:
- Create a Lustre configuraton using your normal methods. No Lustre clients
need be mounted.
- Determine the obdfilter instance names on the OSS(s), column 4
of 'lctl dl'.  In this example we wish to exercise OSTs on two OSSs:

# pdsh -w oss[01-02] lctl dl |grep obdfilter |sort
oss01:   0 UP obdfilter oss01-sdb oss01-sdb_UUID 3
oss01:   2 UP obdfilter oss01-sdd oss01-sdd_UUID 3
oss02:   0 UP obdfilter oss02-sdi oss02-sdi_UUID 3
...

Here the obdfilter instance names are oss01-sdb, oss01-sdd, oss02-sdi.

Since you are driving obdfilter instances directly, set the shell variable
'targets' to the names of the obdfilter instances. Obdfilter instances
residing on remote nodes should be specified in "hostname:OSTname" format.

Example:

If the script is invoked on a node other than oss01 or oss02:

targets='oss01:oss01-sdb oss01:oss01-sdd oss02:oss02-sdi' \
   nobjhi=2 thrhi=2 size=1024 case=disk sh obdfilter-survey

or if the script is invoked on oss01:

targets='oss01-sdb oss01-sdd oss02:oss02-sdi' \
   nobjhi=2 thrhi=2 size=1024 case=disk sh obdfilter-survey


To run against a network:
------------------------
- It is suggested that there should be passwordless entry between client
  and server machine to avoid typing password.

1. Automated run:
- You do not need to configure Lustre for case=network, you only need
to load the Lustre modules. It is possible to have the server devices UP,
the obdfilter-survey does not touch these existing devices.
It creates the tested server device and cleanups it at the end.
The obdfilter-survey is invoked from a client node, but the device
list on client node must be empty, use lctl to check the device list.

e.g.
    $ lctl dl
    $

Invoke obdfilter-survey from a client node and pass parameter case=network and
targets="<hostname/ip_of_server>" to the script in addition to any
customization variables.

e.g. $ nobjhi=2 thrhi=2 size=1024 targets="10.10.2.1" \
       case=network sh obdfilter-survey
or
     $ nobjhi=2 thrhi=2 size=1024 targets="oss01" \
       case=network sh obdfilter-survey

On server side you can see the stats at :
        /proc/fs/lustre/obdecho/echo_srv/stats
where, 'echo_srv' is the obdecho server created through script.

NOTE: In network test only automated run is supported.


To run against network-disk:
----------------------------
1. Automated run:
- Create a Lustre configuration using your normal methods. Mount the
Lustre filesystem on the client node where obdfilter-survey will be invoked.
Please make sure that there is only one Lustre client is mounted
on the client node.
On the Lustre client node, invoke the obdfilter-survey script with
parameter case=netdisk.
e.g. : $ nobjhi=2 thrhi=2 size=1024 case=netdisk sh obdfilter-survey

2. Manual run:
- Create a Lustre configuration using your normal methods. Mount the
Lustre filesystem on the client node where obdfilter-survey will be invoked.
- Determine the OSCs on the local client. Column 4 of 'lctl dl' output

Example:
# lctl dl | awk '/osc/{print $4}'
iokit2-OST0003-osc-ffff8801eee34000
...

Set the shell variable 'targets' to the names of the OSCs under test.
# targets="iokit2-OST0003-osc-ffff8801eee34000" \
    nobjhi=2 thrhi=2 size=1024 case=netdisk sh obdfilter-survey


Output files:
-------------

When the script runs, it creates a number of working files and a pair of
result files.  All files start with the prefix given by the customization
variable ${result_prefix}.

${result_prefix}.summary           same as stdout
${result_prefix}.script_*          per-host test script files
${result_prefix}.detail_tmp*       per-ost result files
${result_prefix}.detail            collected result files for post-mortem

The script iterates over the given numbers of threads, objects and record sizes.
It performs all the specified tests and checks that all test processes
completed successfully.

Note that the script may not clean up properly if it is aborted or if it
encounters an unrecoverable error.  In this case, manual cleanup may be
required, possibly including killing any running instances of 'lctl' (local
or remote), removing echo_client instances created by the script and
unloading obdecho.

Example:
- use lctl to see the devices created by obdfilter-survey

e.g. $ lctl dl
       0 UP obdecho echo_srv echo_srv_UUID 3
       1 UP ost OSS OSS_UUID 3

- cleanup OSS device

e.g. $ lctl << EOF
     > cfg_device OSS
     > cleanup
     > detach
     > EOF

- cleanup echo_srv device

e.g. $ lctl << EOF
     > cfg_device echo_srv
     > cleanup
     > detach
     > EOF



Script output
-------------

The summary file and stdout contain lines like...

ost 8 sz 67108864K rsz 1024 obj    8 thr    8 write  613.54 [ 64.00, 82.00]

ost 8          is the total number of OSTs under test.
sz 67108864K   is the total amount of data read or written (in KB).
rsz 1024       is the record size (size of each echo_client I/O, in KB).
obj    8       is the total number of objects over all OSTs
thr    8       is the total number of threads over all OSTs and objects
write          is the test name.  If more tests have been specified they
               all appear on the same line.
613.54         is the aggregate bandwidth over all OSTs measured by
               dividing the total number of MB by the elapsed time.
[64.00, 82.00] are the minimum and maximum instantaneous bandwidths seen on
               any individual OST.

Note that although the numbers of threads and objects are specifed per-OST
in the customization section of the script, results are reported aggregated
over all OSTs.

Visualising Results
-------------------

I've found it most useful to import the summary data (it's fixed width)
into gnuplot, Excel (or any graphing package) and graph bandwidth v.
# threads for varying numbers of concurrent regions.  This shows how
the OSS performs for a given number of concurrently accessed objects
(i.e. files) with varying numbers of I/Os in flight.

It is also extremely useful to record average disk I/O sizes during each
test.  These numbers help find pathologies in file the file system block
allocator and the block device elevator.

The included plot-obdfilter script is an example of processing the output
files to a .csv format and plotting graph using gnuplot.
